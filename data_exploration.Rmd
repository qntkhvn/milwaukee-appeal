---
title: "Data Exploration"
output: 
  pdf_document: 
    number_sections: yes
    highlight: tango
fontsize: 10pt
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE,
                      comment = "",
                      fig.height = 4,
                      fig.width = 6,
                      fig.align = "center")
```

# Packages

Make sure to install the following packages (for now... we're gonna need a few more)

```{r, eval = FALSE}
# run this code to install
install.packages(c("tidyverse", "janitor", "scales", "ggmap", "naniar"))
```

You only need to **install** a package **ONCE**.

But every time a new R session is opened, be sure to **load** the packages. (using the `library()` function)

```{r}
library(tidyverse)
library(janitor)
library(scales)
theme_set(theme_light()) # set theme for ggplot2 plotting, you don't have to do this
```

\newpage

# Data

```{r}
# be sure to change the file path
# your path is different than mine
load("~/Desktop/consulting/project/consulting_data.RData") # "final"
load("~/Desktop/consulting/project/location_data.RData") # "final_loc"
load("~/Desktop/consulting/project/nbhd_map.RData") # "nbhd_map"
```

These 3 files should appear in Environment tab of the upper right box of RStudio.

## Location

As I mentioned before, I took the location (`final_loc`) data and did some cleaning and created new variables (latitiude, longitude). 

Here's the cleaning code if you're interested (this took awhile to run, like hours, so don't run it, unless you want to test how fast your computer is.)

```{r, eval = FALSE}
geo_cleaned <- final_loc %>%
  
  # this function is from the janitor package
  # here it converts the column names from upper to lower case
  clean_names() %>%
  
  # converts property id's to character
  mutate(prop_id = as.character(prop_id)) %>%
  
  # only keeps ids that belong to the appeals data
  filter(prop_id %in% as.character(final$PROP_ID)) %>% 
  
  # make sure all zip are filled in
  fill(zip) %>%
  
  # use string methods to remove excess information (unecessary characters, etc.) from address
  # also paste the address with city, state, and zip code 
  mutate(address = str_remove(address, ",.*"),
         address = str_remove(address, "--[:upper:]|-\\]"),
         address = str_remove(address, ".*-"),
         address = str_c(address, "Milwaukee", "WI", zip, sep = ", ")) %>%
  
  # performs geocoding to get the latitude/longitude for every observation
  # we're using the ArcGIS API, which takes awhile to run
  tidygeocoder::geocode(address, method = "arcgis")

# write to a csv file after the process is complete
# write_csv("geo_cleaned.csv")
```

I saved this data to a `.csv` file, and the following code will import this data for you

```{r}
geo_cleaned <- read_csv("https://www.dropbox.com/s/9jcsk1477qc4k60/geo_cleaned.csv?dl=1") %>% 
  mutate(prop_id = as.character(prop_id)) # convert prop_id to a character, for joining purposes
```

\newpage

## Main dataset

Now onto the main dataset (`final`).

I did exploration on the 3 appealed columns (`appealed19`, `appealed20`, `appealed21`), and see that each one of them is either `TRUE` (appealed) or `NA` (missing data). So I double-checked with Colin, and we can treat `NA` as a not appealed.

I'm also going to create a new column called `appealed`, which is an indicator for the event "at least one of {`appealed19`, `appealed20`, `appealed21`} is true."

```{r}
housing <- final %>% 
  clean_names() %>% 
  mutate(prop_id = as.character(prop_id), # convert to character
         
         # turns missing appeal value to yes/no
         appealed19 = ifelse(is.na(appealed19), "no", "yes"), 
         appealed20 = ifelse(is.na(appealed20), "no", "yes"),
         appealed21 = ifelse(is.na(appealed21), "no", "yes"),
         
         # if at least one of appealed19, 20, 21 is yes, then yes.
         appealed = ifelse(appealed19 == "yes" | appealed20 == "yes" | appealed21 == "yes", 
                           "yes", "no"),
         
         # remove excess characters from these variables
         bld_type = str_remove(bld_type, ".*- "),
         nbhd = str_sub(nbhd, 1, 4),
         qual = str_remove(qual, " -.*"),
         cond = str_remove(cond, ".*- "),
         kitchen_rating = str_remove(kitchen_rating, ".*- "),
         full_bath_rating = str_remove(full_bath_rating, ".*- "),
         half_bath_rating = str_remove(half_bath_rating, ".*- ")) %>% 
  
  # finally, join with the location data that I cleaned/created
  left_join(geo_cleaned, by = "prop_id")
```

For now, let's not use the third dataset (`nbhd_map`). We can come back and use it to make maps later.

\newpage

# EDA

## Categorical variables

### Neighborhood

```{r}
housing %>% 
  
  # get the count, number of appealed, and % appealed for each neighborhood
  group_by(nbhd) %>%  
  summarize(n = n(),
            n_appealed = sum(appealed == "yes"),
            pct_appealed = n_appealed / n) %>%
  
  # sort the % appealed by descending order
  arrange(desc(pct_appealed)) %>%  
  
  # get the top 15 neighborhood
  slice_head(n = 15) %>%  
  
  # reorder neighborhood, for plotting purposes
  mutate(nbhd = fct_reorder(nbhd, pct_appealed)) %>%
  
  # initialize a plot
  ggplot(aes(x = pct_appealed, y = nbhd)) +
  
  # add a bar graph (col stands for "column")
  geom_col() +
  
  # relabel the x-axis to percent (from proportion)
  scale_x_continuous(labels = percent) +
  
  # change x, y labels, add title
  labs(x = "% appealed",
       y = "Neighborhood Code",
       title = "Top 15 neighborhoods with highest property appeal rate")
```

4580 seem to have higher appeal rate than other.

\newpage

### Zip code

Use the same code as neighborhood, just change the variable

```{r}
housing %>% 
  group_by(zip) %>% 
  summarize(n = n(),
            n_appealed = sum(appealed == "yes"),
            pct_appealed = n_appealed / n) %>%
  arrange(desc(pct_appealed)) %>% 
  slice_head(n = 15) %>% 
  mutate(zip = fct_reorder(as.character(zip), pct_appealed)) %>% 
  ggplot(aes(x = pct_appealed, y = zip)) +
  geom_col() +
  scale_x_continuous(labels = percent) +
  labs(x = "% appealed",
       y = "Neighborhood Code",
       title = "Top 15 zip codes with highest property appeal rate")
```

53217, followed by 53212, have higher appeal rate than other zip codes.

\newpage

**HELPER FUNCTIONS**

We could play the same game with other categorical variables, but let's first write some helper functions, so that we don't have to copy the same code over and over again

**Summarize appeal rate by group**
```{r}
summarize_appealed <- function(tbl) {
  tbl %>%
    summarize(n = n(),
              n_appealed = sum(appealed == "yes"),
              pct_appealed = n_appealed / n) %>%
    arrange(desc(n))
}
```

**Plotting a bar graph of appealed vs a categorical variable**

```{r}
# dplyr programming
# https://dplyr.tidyverse.org/articles/programming.html
# categorical plot
# function takes in df and the name of categorical feature
plot_appealed <- function(df, cat_feat, ...) {
  if (!is.factor(pull(df, {{ cat_feat }}))) {
    df <- df %>%
      mutate({{ cat_feat }} := fct_reorder({{ cat_feat }}, pct_appealed))
  }
  
  df %>%
    filter(!is.na({{ cat_feat }})) %>% 
    ggplot(aes(pct_appealed, {{ cat_feat }}, ...)) +
    geom_col() +
    scale_x_continuous(labels = percent) +
    labs(x = "% appealed")
}
```

\newpage

### Building Type

Let's try the 2 helper functions

```{r}
housing %>% 
  group_by(bld_type) %>% 
  summarize_appealed() %>% 
  plot_appealed(bld_type)
```

Contemporary and mansion are the 2 building types with high appeal rate... interesting.

OK... do the same thing with other categorical variables in the dataset, if you want to play around

\newpage

## Numerical variables

```{r}
housing %>% 
  select(appealed, finished_area, land_sf, sale_price) %>% 
  pivot_longer(!appealed) %>% 
  ggplot(aes(value, fill = appealed)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ name, scales = "free", ncol = 1) +
  scale_x_log10() +
  labs(title = "Density for some numeric variables (on a log10 scale)")
```

\newpage

## Map

Let's try a map

```{r, fig.width = 5}
# aggregate data by latitude and longitude
lat_long <- housing %>% 
  group_by(lat = round(lat, 2),
           long = round(long, 2)) %>%
  summarize_appealed()

lat_long %>% 
  ggplot(aes(long, lat, color = pct_appealed)) + # color the points by % appealed
  geom_point(alpha = 0.5) +
  scale_color_gradient2(low = "darkblue",
                        high = "darkred",
                        midpoint = 0.25) +
  labs(title = "A simple map, made with geom_point()")
```

Some places have higher appeal rate than other. Location (lat/long) could be helpful in predicting appealed outcome.

\newpage

Let's try `ggmap`

```{r}
library(ggmap)

# map range
map_range <- c(left = min(housing$long, na.rm = TRUE) - 0.01,
               bottom = min(housing$lat, na.rm = TRUE) - 0.01,
               right = max(housing$long, na.rm = TRUE) + 0.01,
               top = max(housing$lat, na.rm = TRUE) + 0.01)

milwaukee_map <- get_stamenmap(map_range, zoom = 12)
```

```{r}
milwaukee_map %>% 
  ggmap() +
  labs(title = "Just a plain map of Milwaukee")
```

\newpage

```{r}
milwaukee_map %>% 
  ggmap() +
  geom_point(aes(long, lat, color = pct_appealed), 
             alpha = 0.5,
             data = lat_long) +
  scale_color_gradient2(low = "darkblue",
                        high = "darkred",
                        midpoint = 0.25) +
  labs(title = "Map of Milwaukee, with points added")
```


\newpage

```{r}
get_stamenmap(map_range,
              zoom = 12,
              maptype = "toner-background") %>% 
  ggmap() +
  geom_point(aes(long, lat, color = pct_appealed), 
             alpha = 0.5,
             data = lat_long) +
  scale_color_gradient2(low = "darkblue",
                        high = "darkred",
                        midpoint = 0.25) +
  labs(title = "Black and white version of the map")
```

\newpage

## Missing Data

```{r}
# a great package for missing data visualization
library(naniar)
```

```{r}
housing %>% 
  gg_miss_var(show_pct = TRUE)
```

Sale price, sale date, half bath rating, half bath count are the columns with lots of missing data (over 75% missing)

\vspace{5cm}

Anyway... here's a start.