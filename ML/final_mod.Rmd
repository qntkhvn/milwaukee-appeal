---
title: "Final Models"
output: html_document
---

# Set up

```{r}
library(tidyverse)
library(janitor)
library(lubridate)
theme_set(theme_light()) 

library(here)
path <- "Desktop/consulting/project"
load(here(path, "consulting_data.RData"))
load(here(path, "location_data.RData"))
# load(here(path, "nbhd_map.RData"))

geo_cleaned <- read_csv("https://www.dropbox.com/s/9jcsk1477qc4k60/geo_cleaned.csv?dl=1") %>% 
  mutate(prop_id = as.character(prop_id),
         zip = as.character(zip))

# properties in the main dataset but are not in the location data
# 207147 207148 212702 212704 222335 224675
final$PROP_ID[!final$PROP_ID %in% final_loc$PROP_ID]

# fill in missing info based on properties nearby

# props close to 207147, 207148
# housing %>% 
#   select(prop_id, zip, lat, long) %>% 
#   filter(str_detect(prop_id, "^2071"))
# 
# # props close to 212702, 212704
# housing %>% 
#   select(prop_id, zip, lat, long) %>% 
#   filter(str_detect(prop_id, "^21270"))
#   
# # props close to 222335
# housing %>% 
#   select(prop_id, zip, lat, long) %>% 
#   filter(str_detect(prop_id, "^22233"))
# 
# # props close to 224675
# housing %>% 
#   select(prop_id, zip, lat, long) %>% 
#   filter(str_detect(prop_id, "^22467"))

geo_miss <- tibble(
  prop_id = c("207147", "207148", "212702", "212704", "222335", "224675"),
  zip = c("53204", "53204", "53215", "53215", "53215", "53215"),
  lat = c(43.024, 43.024, 43.0106, 43.0106, 42.997, 42.9978),
  long = c(-87.91378, -87.91378, -87.948, -87.948, -87.94965, -87.9259)
)

geo_final <- geo_cleaned %>% 
  bind_rows(geo_miss)


# cleaning by column order
housing <- final %>%
  clean_names() %>%
  mutate(
    
    prop_id = as.character(prop_id),
    
    bld_type = str_remove(bld_type, ".*- "),
    
    appraiser = str_sub(appraiser, start = 7),
    
    nbhd = str_sub(nbhd, 1, 4),
    
    qual = ifelse(str_detect(qual, "M"), "C", str_remove(qual, " -.*")),
    
    cond = str_remove(cond, ".*- "),
    
    kitchen_rating = str_remove(kitchen_rating, ".*- "),
    kitchen_rating = ifelse(kitchen_ct == 0, NA, kitchen_rating),
    
    
    full_bath_ct = ifelse(is.na(full_bath_ct), 0, full_bath_ct),
    full_bath_rating = str_remove(full_bath_rating, ".*- "),
    full_bath_rating = ifelse(full_bath_ct == 0, NA, full_bath_rating),
    
    half_bath_ct = ifelse(is.na(half_bath_ct), 0, half_bath_ct),
    half_bath_rating = str_remove(half_bath_rating, ".*- "),
    half_bath_rating = ifelse(half_bath_ct == 0, NA, half_bath_rating),
    
    year_built = ifelse(year_built == 0, NA, year_built),
    
    finished_area = ifelse(finished_area == 0, NA, finished_area),
    
    land_sf = ifelse(land_sf == 0, NA, land_sf),
    
    #sale_month = ifelse(is.na(sale_date), 0, month(sale_date)), # month
    sale_year = ifelse(is.na(sale_date), 0, year(sale_date)), # year
    # sale_wday = ifelse(is.na(sale_date), 0, wday(sale_date)), # day of the week
    # sale_week = ifelse(is.na(sale_date), 0, week(sale_date)), # week number (1-52)
    # sale_year = ifelse(sale_year == 2021 & sale_month > 8, 2020, sale_year),
    
    sale_status = ifelse(sale_year < 2020, 1, 0),
    
    # sale_price = ifelse(is.na(sale_price), 0, sale_price),
    sale_price = case_when(
      sale_price <= 100000 ~ "<100k",
      sale_price > 100000 & sale_price <= 150000 ~ "100-150k",
      sale_price > 150000 & sale_price <= 200000 ~"150-200k",
      sale_price > 200000 ~ ">200k",
      is.na(sale_price) ~ "no_sale"),   # several sale price of 0
    
    appealed19 = ifelse(is.na(appealed19), 0, 1), 
    appealed20 = ifelse(is.na(appealed20), "no", "appealed"),
    appealed21 = ifelse(is.na(appealed21), "no", "yes"),
    
  ) %>% 
  left_join(geo_final, by = "prop_id") # finally, joined with latitude/longitude data


# ordinal encoding
rating_levels <- c("Excellent", "Very Good", "Good", "Average", 
                   "Fair", "Poor", "Very Poor", "Unsound")
quality <- tibble(
  qual = c("AA+", "AA", "AA-", "A+", "A", "A-", "B+", "B", "B-", 
           "C+", "C", "C-", "D+", "D", "D-", "E+", "E", "E-"),
  qual_score = c(2.75, 2.63, 2.5, 2.35, 2, 1.65, 1.34, 1.14, 1.075, 1.04, 
                 1.015, 0.985, 0.955, 0.925, 0.895, 0.65, 0.55, 0.45))

raw <- housing %>% 
  left_join(quality) %>% 
  mutate(qual_score = ifelse(is.na(qual_score), 0, qual_score), 
         cond = as.numeric(factor(cond, levels = rev(rating_levels))),
         cond = ifelse(is.na(cond), 0, cond),
         kitchen_rating = as.numeric(factor(kitchen_rating, levels = rev(rating_levels))),
         kitchen_rating = ifelse(is.na(kitchen_rating), 0 , kitchen_rating),
         full_bath_rating = as.numeric(factor(full_bath_rating, levels = rev(rating_levels))),
         full_bath_rating = ifelse(is.na(full_bath_rating), 0 , full_bath_rating),
         half_bath_rating = as.numeric(factor(half_bath_rating, levels = rev(rating_levels))),
         half_bath_rating = ifelse(is.na(half_bath_rating), 0 , half_bath_rating),
         finished_area = log1p(finished_area),
         land_sf = log1p(land_sf)) %>% 
  select(-sale_date, -sale_price, -appealed21, -sale_year, -qual, -address, -geo_tract) %>%
  mutate(across(where(is.character), factor))


# how many are still missing?
# only 9
raw %>% 
  anti_join(drop_na(raw))

# 2 with missing years
# fill in with mode
# new function shortcut
get_mode <- \(v) unique(v)[which.max(tabulate(match(v, unique(v))))]
mode_year <- get_mode(raw$year_built)


# missing finished and land sf
# fill in with mean
avg_fa <- mean(raw$finished_area, na.rm = TRUE)
avg_lsf <- mean(raw$land_sf, na.rm = TRUE)

raw <- raw %>% 
  mutate(year_built = ifelse(is.na(year_built), mode_year, year_built),
         finished_area = ifelse(is.na(finished_area), avg_fa, finished_area),
         land_sf = ifelse(is.na(land_sf), avg_lsf, land_sf),
         zip = fct_lump_min(zip, min = 1000),
         bld_type = fct_lump_min(bld_type, min = 1000)) %>% 
  select(-prop_id)

library(tidymodels)

# 70-30 train-test split
set.seed(100)
split <- raw %>% 
  initial_split(prop = 0.7, strata = appealed20)
train <- training(split)
test <- testing(split)

# k-fold cross-validation
# change v for the number of folds
set.seed(101)
folds <- train %>% 
  vfold_cv(v = 5, strata = appealed20)
```

# LASSO

```{r}
# start with a LASSO

# set up

# specification
lasso_spec <- logistic_reg(penalty = tune()) %>%
  set_engine("glmnet")

# recipe
lasso_rec <- 
  recipe(appealed20 ~ ., data = train) %>% 
#step_other(bld_type, zip, nbhd, threshold = 0.01) %>% 
# step_unknown(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% 
  step_normalize(all_numeric_predictors())
  #themis::step_smote(appealed20, seed = 1, over_ratio = 0.1)
# step_impute_median(all_numeric_predictors()) %>% 

# workflow
lasso_wf <- workflow() %>% 
  add_model(lasso_spec) %>% 
  add_recipe(lasso_rec)

##########################################################################

# tune


# grid
grid_control <- control_grid(save_pred = TRUE,
                             save_workflow = TRUE,
                             extract = extract_model)

# tuning
set.seed(102)
lasso_tune <-
  tune_grid(object = lasso_wf,
            resamples = folds,
            metrics = metric_set(roc_auc),
            control = grid_control,
            grid = grid_regular(penalty(), levels = 50))

lasso_tune %>% 
  autoplot() +
  geom_vline(xintercept = 0.000339, 
             linetype = "dashed",
             size = 0.1)

# cv auc 0.7468982
lasso_tune %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  slice_head(n = 1) 

# obtain the best tuning parameter(s)
lasso_best <- lasso_tune %>% 
  select_best("roc_auc")

lasso_best
# pen 0.000339

# finalize the model based on the selected best/optimal hyperparam values
lasso_final <- lasso_wf %>% 
  finalize_workflow(lasso_best)

# fit a final model on the entire train set
lasso_last <- lasso_final %>% 
  last_fit(split)

# evaluate (on the test set)

lasso_last %>% 
  collect_predictions() %>% 
  roc_curve(appealed20, .pred_appealed) %>% 
  ggplot(aes(1 - specificity, sensitivity)) +
  geom_abline() +
  geom_path() +
  coord_fixed()

lasso_last %>% 
  collect_predictions() %>% 
  roc_auc(appealed20, .pred_appealed)

# AUC = 0.7520328 on test set

lasso_fit_final <- lasso_final %>% 
  fit(raw)

lasso_pred <- lasso_fit_final %>% 
  augment(raw)
```

# XGBoost

```{r}
xg_spec <- boost_tree(trees = tune(),
                      tree_depth = tune(), 
                      learn_rate = tune(),
                      min_n = tune(),
                      sample_size = 0.5,
                      mtry = tune()) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

xg_rec <- recipe(appealed20 ~ ., data = train) %>% 
  step_dummy(all_nominal_predictors(), one_hot = TRUE)
  #themis::step_smote(appealed20, seed = 2, over_ratio = 0.1)

# workflow
xg_wf <- workflow() %>% 
  add_model(xg_spec) %>% 
  add_recipe(xg_rec)


xg_grid <- grid_latin_hypercube(
  trees(range = c(50, 500)),
  tree_depth(range = c(4, 8)),
  min_n(range = c(3, 15)),
  finalize(mtry(range = c(3, 15)), train),
  #loss_reduction(),
  # sample_size = sample_prop(),
  learn_rate(range = c(0.005, 0.02), trans = NULL),
  size = 50
)

grid_control <- control_grid(save_pred = TRUE,
                             save_workflow = TRUE,
                             extract = extract_model)

# doParallel::registerDoParallel(cores = 5)
# set.seed(103)
# xg_tune <- xg_wf %>% 
#   tune_grid(resamples = folds,
#             metrics = metric_set(roc_auc),
#             control = grid_control,
#             grid = xg_grid)

# write_rds(xg_tune, "~/Desktop/mke/xg_tune_final.rds")

xg_tune <- read_rds("~/Desktop/mke/xg_tune_final.rds")

xg_tune %>% 
  autoplot()

# cv auc 0.761449
xg_tune %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  slice_head(n = 1)

xg_best <- xg_tune %>% 
  select_best("roc_auc")

xg_best

xg_final <- xg_wf %>% 
  finalize_workflow(xg_best)

xg_final


xg_last <- xg_final %>% 
  last_fit(split)

xg_last %>% 
  collect_predictions() %>% 
  roc_curve(appealed20, .pred_appealed) %>% 
  ggplot(aes(1 - specificity, sensitivity)) +
  geom_abline() +
  geom_path() +
  coord_fixed()

# auc on test 0.7706084
xg_last %>% 
  collect_predictions() %>% 
  roc_auc(appealed20, .pred_appealed)

xg_fit_final <- xg_final %>% 
  fit(raw)

xg_pred <- xg_fit_final %>% 
  augment(raw)

# for shiny app
# write_csv(mutate(housing, prob_appealed = xg_pred$.pred_appealed), "pred.csv")

# housing %>% 
#   select(prop_id) %>% 
#   mutate(prob_appealed = xg_pred$.pred_appealed) %>% 
#   write_rds("appealed_pred.rds")
```

# Model comp

```{r}
xg_last %>% 
  collect_predictions() %>% 
  select(.pred_appealed, appealed20) %>% 
  mutate(model = "XGBoost") %>% 
  bind_rows(
    lasso_last %>% 
      collect_predictions() %>% 
      select(.pred_appealed, appealed20) %>% 
      mutate(model = "LASSO")
  ) %>%
  group_by(model) %>% 
  roc_curve(appealed20, .pred_appealed) %>% 
  autoplot()
```

# Feature importance

```{r}
xg_mod <- extract_fit_parsnip(xg_fit_final)

library(vip)
vip(xg_mod$fit)

X <- prep(xg_rec, train) %>% 
  juice() %>% 
  select(-appealed20) %>% 
  as.matrix()

# library(fastshap)
# shap <- explain(xg_mod$fit, X = X, exact = TRUE)
# autoplot(shap)

library(SHAPforxgboost)
xg_shap <- shap.prep(xg_mod$fit, X_train = X)


shap.importance(xg_shap, names_only = TRUE)
shap.plot.summary.wrap1(xg_mod$fit, X, top_n = 15)

shap.importance(xg_shap) %>% 
  as_tibble() %>% 
  slice_max(n = 15, order_by = mean_abs_shap) %>% 
  mutate(variable = fct_reorder(variable, mean_abs_shap)) %>% 
  ggplot(aes(mean_abs_shap, variable)) +
  geom_col() +
  ylab(NULL) +
  xlab("Mean(|SHAP|)")

```


